{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cfe4b80e-fbe3-44a9-af13-c52c563b862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames loaded from pickle files.\n",
      "First few rows of train_df:\n",
      "        Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
      "0   914403  B0009W5KHM   AV6QDP8Q0ONK4                     2   \n",
      "1   354887  6303079709  A2I8RXJN80A2D2                     0   \n",
      "2  1407653  B004H0M2XC  A3FHV3RV8Z12E6                     0   \n",
      "3  1377458  B003ZJ9536  A12VLTA3ZHVPUY                     1   \n",
      "4   475323  630574453X  A13NM1PES9OXVN                     2   \n",
      "\n",
      "   HelpfulnessDenominator        Time  \\\n",
      "0                       2  1341014400   \n",
      "1                       0  1168819200   \n",
      "2                       0  1386201600   \n",
      "3                       1  1348704000   \n",
      "4                       3   970012800   \n",
      "\n",
      "                                         Summary  \\\n",
      "0                                  GOOD FUN FILM   \n",
      "1                                   Movie Review   \n",
      "2             When is it a good time to Consent?   \n",
      "3                                          TRUTH   \n",
      "4  Intelligent and bittersweet -- stays with you   \n",
      "\n",
      "                                                Text  Score  \n",
      "0  While most straight to DVD films are not worth...    5.0  \n",
      "1  I have wanted this one for sometime, also.  I ...    5.0  \n",
      "2  Actually this was a pretty darn good indie fil...    4.0  \n",
      "3  Episodes 37 to 72 of the series press on in a ...    5.0  \n",
      "4  I was really impressed with this movie, but wa...    3.0  \n",
      "\n",
      "First few rows of test_df:\n",
      "        Id  Score\n",
      "0  1323432    NaN\n",
      "1  1137299    NaN\n",
      "2  1459366    NaN\n",
      "3   931601    NaN\n",
      "4  1311995    NaN\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "train_csv_path = 'train.csv'\n",
    "test_csv_path = 'test.csv'\n",
    "train_pickle_path = 'train.pkl'\n",
    "test_pickle_path = 'test.pkl'\n",
    "\n",
    "# Check if pickle files exist\n",
    "if os.path.exists(train_pickle_path) and os.path.exists(test_pickle_path):\n",
    "    # Load DataFrames from pickle files\n",
    "    with open(train_pickle_path, 'rb') as f:\n",
    "        train_df = pickle.load(f)\n",
    "    with open(test_pickle_path, 'rb') as f:\n",
    "        test_df = pickle.load(f)\n",
    "    print(\"DataFrames loaded from pickle files.\")\n",
    "else:\n",
    "    # Load DataFrames from CSV files\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    print(\"DataFrames loaded from CSV files.\")\n",
    "\n",
    "    # Save DataFrames to pickle files for future use\n",
    "    with open(train_pickle_path, 'wb') as f:\n",
    "        pickle.dump(train_df, f)\n",
    "    with open(test_pickle_path, 'wb') as f:\n",
    "        pickle.dump(test_df, f)\n",
    "    print(\"DataFrames saved to pickle files for future use.\")\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "print(\"First few rows of train_df:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Display the first few rows of the test data\n",
    "print(\"\\nFirst few rows of test_df:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8d4b4877-13d9-48bd-9343-9be2e5fcadc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_df:\n",
      "Id                             0\n",
      "ProductId                      0\n",
      "UserId                         0\n",
      "HelpfulnessNumerator           0\n",
      "HelpfulnessDenominator         0\n",
      "Time                           0\n",
      "Summary                       32\n",
      "Text                          62\n",
      "Score                     212192\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test_df:\n",
      "Id            0\n",
      "Score    212192\n",
      "dtype: int64\n",
      "\n",
      "Data types in train_df:\n",
      "Id                          int64\n",
      "ProductId                  object\n",
      "UserId                     object\n",
      "HelpfulnessNumerator        int64\n",
      "HelpfulnessDenominator      int64\n",
      "Time                        int64\n",
      "Summary                    object\n",
      "Text                       object\n",
      "Score                     float64\n",
      "dtype: object\n",
      "\n",
      "Data types in test_df:\n",
      "Id         int64\n",
      "Score    float64\n",
      "dtype: object\n",
      "\n",
      "Number of duplicate rows in train_df: 0\n",
      "\n",
      "Statistical summary of numerical features in train_df:\n",
      "                 Id  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "count  1.697533e+06          1.697533e+06            1.697533e+06   \n",
      "mean   8.487660e+05          3.569048e+00            5.301422e+00   \n",
      "std    4.900357e+05          1.727883e+01            2.024445e+01   \n",
      "min    0.000000e+00          0.000000e+00            0.000000e+00   \n",
      "25%    4.243830e+05          0.000000e+00            0.000000e+00   \n",
      "50%    8.487660e+05          1.000000e+00            1.000000e+00   \n",
      "75%    1.273149e+06          3.000000e+00            5.000000e+00   \n",
      "max    1.697532e+06          6.084000e+03            6.510000e+03   \n",
      "\n",
      "               Time         Score  \n",
      "count  1.697533e+06  1.485341e+06  \n",
      "mean   1.262422e+09  4.110517e+00  \n",
      "std    1.289277e+08  1.197651e+00  \n",
      "min    8.793792e+08  1.000000e+00  \n",
      "25%    1.164413e+09  4.000000e+00  \n",
      "50%    1.307491e+09  5.000000e+00  \n",
      "75%    1.373242e+09  5.000000e+00  \n",
      "max    1.406074e+09  5.000000e+00  \n",
      "\n",
      "Value counts for 'Score' in train_df:\n",
      "Score\n",
      "5.0    793163\n",
      "4.0    335228\n",
      "NaN    212192\n",
      "3.0    176082\n",
      "1.0     91190\n",
      "2.0     89678\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of rows with missing 'Score' in train_df: 212192\n",
      "\n",
      "After removing missing 'Score', the shape of train_df:\n",
      "(1485341, 9)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in train_df\n",
    "print(\"Missing values in train_df:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for missing values in test_df\n",
    "print(\"\\nMissing values in test_df:\")\n",
    "print(test_df.isnull().sum())\n",
    "# Examine data types in train_df\n",
    "print(\"\\nData types in train_df:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "# Examine data types in test_df\n",
    "print(\"\\nData types in test_df:\")\n",
    "print(test_df.dtypes)\n",
    "# Check for duplicates in train_df\n",
    "duplicate_rows = train_df[train_df.duplicated()]\n",
    "print(f\"\\nNumber of duplicate rows in train_df: {duplicate_rows.shape[0]}\")\n",
    "# Statistical summary of numerical features\n",
    "print(\"\\nStatistical summary of numerical features in train_df:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Statistical summary of categorical features\n",
    "print(\"\\nValue counts for 'Score' in train_df:\")\n",
    "print(train_df['Score'].value_counts(dropna=False))\n",
    "# Separate rows with missing 'Score' in train_df\n",
    "missing_score_df = train_df[train_df['Score'].isnull()]\n",
    "print(f\"\\nNumber of rows with missing 'Score' in train_df: {missing_score_df.shape[0]}\")\n",
    "\n",
    "# Keep only rows with non-missing 'Score' for training\n",
    "train_df = train_df[train_df['Score'].notnull()]\n",
    "\n",
    "# Convert 'Score' to integer type\n",
    "train_df['Score'] = train_df['Score'].astype(int)\n",
    "print(\"\\nAfter removing missing 'Score', the shape of train_df:\")\n",
    "print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c1d3e42e-fdf5-4c9a-bc74-f4ad38f4cd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing 'Text' values in train_df: 54\n",
      "After removing missing 'Text' rows, the shape of train_df: (1485287, 9)\n",
      "Number of missing 'Summary' values in train_df after imputation: 0\n",
      "\n",
      "First few 'ReviewTime' entries:\n",
      "0   2012-06-30\n",
      "1   2007-01-15\n",
      "2   2013-12-05\n",
      "3   2012-09-27\n",
      "4   2000-09-27\n",
      "Name: ReviewTime, dtype: datetime64[ns]\n",
      "\n",
      "First few rows of time-based features:\n",
      "   ReviewYear  ReviewMonth  ReviewDayOfWeek  ReviewAgeDays\n",
      "0        2012            6                5           4502\n",
      "1        2007            1                0           6495\n",
      "2        2013           12                3           3979\n",
      "3        2012            9                3           4413\n",
      "4        2000            9                2           8796\n",
      "\n",
      "First few 'HelpfulnessRatio' values:\n",
      "0    1.000000\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    1.000000\n",
      "4    0.666667\n",
      "Name: HelpfulnessRatio, dtype: float64\n",
      "\n",
      "First few 'ReviewText' entries:\n",
      "0    GOOD FUN FILM While most straight to DVD films...\n",
      "1    Movie Review I have wanted this one for someti...\n",
      "2    When is it a good time to Consent? Actually th...\n",
      "3    TRUTH Episodes 37 to 72 of the series press on...\n",
      "4    Intelligent and bittersweet -- stays with you ...\n",
      "Name: ReviewText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Number of missing 'Text' values after removing missing 'Score' rows\n",
    "missing_text_count = train_df['Text'].isnull().sum()\n",
    "print(f\"Number of missing 'Text' values in train_df: {missing_text_count}\")\n",
    "# Remove rows with missing 'Text'\n",
    "train_df = train_df[train_df['Text'].notnull()]\n",
    "print(f\"After removing missing 'Text' rows, the shape of train_df: {train_df.shape}\")\n",
    "# Fill missing 'Summary' values with empty string\n",
    "train_df['Summary'] = train_df['Summary'].fillna('')\n",
    "\n",
    "# Verify that there are no missing 'Summary' values\n",
    "missing_summary_count = train_df['Summary'].isnull().sum()\n",
    "print(f\"Number of missing 'Summary' values in train_df after imputation: {missing_summary_count}\")\n",
    "# Convert 'Time' from UNIX timestamp to datetime\n",
    "train_df['ReviewTime'] = pd.to_datetime(train_df['Time'], unit='s')\n",
    "\n",
    "# Display the first few 'ReviewTime' entries\n",
    "print(\"\\nFirst few 'ReviewTime' entries:\")\n",
    "print(train_df['ReviewTime'].head())\n",
    "# Extract year, month, day of week\n",
    "train_df['ReviewYear'] = train_df['ReviewTime'].dt.year\n",
    "train_df['ReviewMonth'] = train_df['ReviewTime'].dt.month\n",
    "train_df['ReviewDayOfWeek'] = train_df['ReviewTime'].dt.dayofweek\n",
    "\n",
    "# Calculate the age of the review in days\n",
    "current_time = pd.to_datetime('now')\n",
    "train_df['ReviewAgeDays'] = (current_time - train_df['ReviewTime']).dt.days\n",
    "\n",
    "# Display the first few rows of new time features\n",
    "print(\"\\nFirst few rows of time-based features:\")\n",
    "print(train_df[['ReviewYear', 'ReviewMonth', 'ReviewDayOfWeek', 'ReviewAgeDays']].head())\n",
    "# Avoid division by zero\n",
    "train_df['HelpfulnessDenominator'] = train_df['HelpfulnessDenominator'].replace(0, np.nan)\n",
    "\n",
    "# Calculate HelpfulnessRatio\n",
    "train_df['HelpfulnessRatio'] = train_df['HelpfulnessNumerator'] / train_df['HelpfulnessDenominator']\n",
    "\n",
    "# Fill NaN values with 0 (assuming no helpfulness feedback)\n",
    "train_df['HelpfulnessRatio'] = train_df['HelpfulnessRatio'].fillna(0.0)\n",
    "\n",
    "# Display the first few rows of 'HelpfulnessRatio'\n",
    "print(\"\\nFirst few 'HelpfulnessRatio' values:\")\n",
    "print(train_df['HelpfulnessRatio'].head())\n",
    "# Combine 'Summary' and 'Text' into 'ReviewText'\n",
    "train_df['ReviewText'] = train_df['Summary'] + ' ' + train_df['Text']\n",
    "\n",
    "# Display the first few 'ReviewText' entries\n",
    "print(\"\\nFirst few 'ReviewText' entries:\")\n",
    "print(train_df['ReviewText'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "13b177e5-d67e-4ad9-b0df-d5dbe3435a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pengrui_f/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed text from pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Text processing libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download NLTK data files \n",
    "nltk.download('stopwords')\n",
    "# Initialize stop words and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and stem words\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "# Check if preprocessed text is already saved\n",
    "preprocessed_text_pickle = 'preprocessed_text.pkl'\n",
    "\n",
    "if os.path.exists(preprocessed_text_pickle):\n",
    "    with open(preprocessed_text_pickle, 'rb') as f:\n",
    "        train_df['CleanReviewText'] = pickle.load(f)\n",
    "    print(\"Loaded preprocessed text from pickle file.\")\n",
    "else:\n",
    "    # Apply preprocessing to 'ReviewText'\n",
    "    train_df['CleanReviewText'] = train_df['ReviewText'].apply(preprocess_text)\n",
    "    print(\"Text preprocessing completed.\")\n",
    "\n",
    "    # Save the preprocessed text to a pickle file\n",
    "    with open(preprocessed_text_pickle, 'wb') as f:\n",
    "        pickle.dump(train_df['CleanReviewText'], f)\n",
    "    print(\"Preprocessed text saved to pickle file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b259574-27b2-4e72-901e-0656adad45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorization completed.\n",
      "TF-IDF features saved to pickle file.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=11000) \n",
    "\n",
    "# Check if TF-IDF features are already saved\n",
    "tfidf_features_pickle = 'tfidf_features.pkl'\n",
    "\n",
    "if os.path.exists(tfidf_features_pickle):\n",
    "    with open(tfidf_features_pickle, 'rb') as f:\n",
    "        X_text_tfidf = pickle.load(f)\n",
    "    print(\"Loaded TF-IDF features from pickle file.\")\n",
    "else:\n",
    "    # Fit and transform the 'CleanReviewText'\n",
    "    X_text_tfidf = tfidf_vectorizer.fit_transform(train_df['CleanReviewText'])\n",
    "    print(\"TF-IDF vectorization completed.\")\n",
    "\n",
    "    # Save the TF-IDF features to a pickle file\n",
    "    with open(tfidf_features_pickle, 'wb') as f:\n",
    "        pickle.dump(X_text_tfidf, f)\n",
    "    print(\"TF-IDF features saved to pickle file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b2f04fc0-fc35-4018-aa78-ee380888b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ReviewLength\n",
    "train_df['ReviewLength'] = train_df['CleanReviewText'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Select numerical features\n",
    "# numerical_features = ['HelpfulnessRatio', 'ReviewLength', 'ReviewYear', 'ReviewMonth', 'ReviewDayOfWeek', 'ReviewAgeDays']\n",
    "numerical_features = ['HelpfulnessRatio', 'ReviewYear']\n",
    "\n",
    "# Extract numerical feature matrix\n",
    "X_numerical = train_df[numerical_features].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6d0d28ca-7086-4d76-8869-d42602ba1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "34019188-6285-44f5-a56d-a5801f3c66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine TF-IDF features and scaled numerical features\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_text_tfidf, X_numerical_scaled])\n",
    "y = train_df['Score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "73679253-9fce-4f10-9638-078a8859f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (1113965, 11002)\n",
      "Validation set size: (371322, 11002)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d93e1866-2bb1-4860-9e1b-076f692c6bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = SGDClassifier(\n",
    "    loss='log_loss',  # For logistic regression\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f6daece7-9bab-43dc-b089-e416d3fba21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6304\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.63      0.56     22797\n",
      "           2       0.36      0.36      0.36     22418\n",
      "           3       0.43      0.35      0.39     44019\n",
      "           4       0.47      0.35      0.40     83804\n",
      "           5       0.75      0.84      0.79    198284\n",
      "\n",
      "    accuracy                           0.63    371322\n",
      "   macro avg       0.50      0.51      0.50    371322\n",
      "weighted avg       0.61      0.63      0.62    371322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ebd01378-61a0-40f3-b3a6-c9d7b1adf12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_pickle = 'trained_model.pkl'\n",
    "with open(model_pickle, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Trained model saved to pickle file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cc84b52f-4d62-495a-bdca-287ed5defa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train data loaded from pickle file.\n",
      "Test data shape after merge: (212192, 9)\n",
      "Loaded preprocessed test text from pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary libraries are imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming that 'model', 'scaler', 'tfidf_vectorizer', 'preprocess_text', 'stop_words', and 'stemmer' are available\n",
    "\n",
    "# 1. Load 'test.csv' if not already loaded\n",
    "test_csv_path = 'test.csv'\n",
    "if 'test_df' not in globals():\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    print(\"Loaded 'test.csv' into test_df.\")\n",
    "\n",
    "# 2. Load the full 'train.csv' to get the test data with features\n",
    "train_full_pickle_path = 'train_full.pkl'\n",
    "if os.path.exists(train_full_pickle_path):\n",
    "    with open(train_full_pickle_path, 'rb') as f:\n",
    "        train_full_df = pickle.load(f)\n",
    "    print(\"Full train data loaded from pickle file.\")\n",
    "else:\n",
    "    train_full_df = pd.read_csv('train.csv')\n",
    "    with open(train_full_pickle_path, 'wb') as f:\n",
    "        pickle.dump(train_full_df, f)\n",
    "    print(\"Full train data loaded from CSV and saved to pickle.\")\n",
    "\n",
    "# 3. Merge test_df with train_full_df to get test_data, preserving the order of test_df\n",
    "test_data = pd.merge(test_df[['Id']], train_full_df, on='Id', how='left')\n",
    "print(f\"Test data shape after merge: {test_data.shape}\")\n",
    "\n",
    "# 4. Handle missing values in 'Text' and 'Summary'\n",
    "test_data['Text'] = test_data['Text'].fillna('')\n",
    "test_data['Summary'] = test_data['Summary'].fillna('')\n",
    "\n",
    "# 5. Combine 'Summary' and 'Text' into 'ReviewText'\n",
    "test_data['ReviewText'] = test_data['Summary'] + ' ' + test_data['Text']\n",
    "\n",
    "# 6. Convert 'Time' from UNIX timestamp to datetime\n",
    "test_data['ReviewTime'] = pd.to_datetime(test_data['Time'], unit='s')\n",
    "\n",
    "# 7. Calculate 'ReviewAgeDays'\n",
    "current_time = pd.to_datetime('now')\n",
    "test_data['ReviewYear'] = test_data['ReviewTime'].dt.year\n",
    "test_data['ReviewAgeDays'] = (current_time - test_data['ReviewTime']).dt.days\n",
    "\n",
    "# 8. Handle 'HelpfulnessDenominator' to avoid division by zero\n",
    "test_data['HelpfulnessDenominator'] = test_data['HelpfulnessDenominator'].replace(0, np.nan)\n",
    "\n",
    "# 9. Calculate 'HelpfulnessRatio'\n",
    "test_data['HelpfulnessRatio'] = test_data['HelpfulnessNumerator'] / test_data['HelpfulnessDenominator']\n",
    "test_data['HelpfulnessRatio'] = test_data['HelpfulnessRatio'].fillna(0.0)\n",
    "\n",
    "# 10. Preprocess 'ReviewText' in test_data\n",
    "preprocessed_test_text_pickle = 'preprocessed_test_text.pkl'\n",
    "\n",
    "if os.path.exists(preprocessed_test_text_pickle):\n",
    "    with open(preprocessed_test_text_pickle, 'rb') as f:\n",
    "        test_data['CleanReviewText'] = pickle.load(f)\n",
    "    print(\"Loaded preprocessed test text from pickle file.\")\n",
    "else:\n",
    "    # Apply preprocessing to 'ReviewText'\n",
    "    test_data['CleanReviewText'] = test_data['ReviewText'].apply(preprocess_text)\n",
    "    print(\"Test text preprocessing completed.\")\n",
    "    \n",
    "    # Save the preprocessed text to a pickle file\n",
    "    with open(preprocessed_test_text_pickle, 'wb') as f:\n",
    "        pickle.dump(test_data['CleanReviewText'], f)\n",
    "    print(\"Preprocessed test text saved to pickle file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "810d6aa5-9f65-45b3-b60f-66e2ef670a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 11. Transform 'CleanReviewText' in test_data using the same TF-IDF vectorizer\n",
    "X_test_text_tfidf = tfidf_vectorizer.transform(test_data['CleanReviewText'])\n",
    "\n",
    "# 12. Extract and standardize numerical features\n",
    "numerical_features = ['HelpfulnessRatio', 'ReviewYear']\n",
    "X_test_numerical = test_data[numerical_features].values\n",
    "X_test_numerical_scaled = scaler.transform(X_test_numerical)\n",
    "\n",
    "# 13. Combine TF-IDF features and scaled numerical features\n",
    "X_test = hstack([X_test_text_tfidf, X_test_numerical_scaled])\n",
    "\n",
    "# 14. Predict 'Score' for test_data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# 15. Create submission DataFrame, ensuring order matches 'test_df'\n",
    "submission_df = test_df.copy()\n",
    "submission_df['Score'] = y_test_pred\n",
    "\n",
    "# 16. Save to 'submission.csv' without index\n",
    "submission_df[['Id', 'Score']].to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69620a3-365a-404f-9012-c6c744005a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
